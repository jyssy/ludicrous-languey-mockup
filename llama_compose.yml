
services:
    init:
        build: .
        restart: always
        volumes:
            - ./models:/app/models
            - ./cache:/app/cache
        environment:
            - HF_TOKEN=${HF_TOKEN}
        deploy:
            resources:
                limits:
                    memory: 100G
                reservations:
                    memory: 90G
        networks:
            - llm_network

    vllm:
        image: drikster80/vllm-gh200-openai
        ipc: host
        restart: always
        volumes:
            - ./cache/huggingface:/root/.cache/huggingface
            - ./models:/root/models
        command: >
            --model /root/models/Llama-3.3-70B-Instruct-FP8-Dynamic
            --served-model-name Llama-3.3-70B-Instruct-FP8-Dynamic
            --max-model-len 32768
            --cpu-only
            --num-scheduler-steps 4
        deploy:
            resources:
                limits:
                    memory: 100G
        depends_on:
            - init
        networks:
            - llm_network

    webui:
        image: ghcr.io/open-webui/open-webui:main
        ports:
            - "8080:8080"
        volumes:
            - open-webui:/app/backend/data
        environment:
            - WEBUI_URL=http://localhost:8080
            - DEFAULT_USER_ROLE=admin
            - ENABLE_SIGNUP=False
            - WEBUI_AUTH=False
            - DEFAULT_MODELS=Llama-3.3-70B-Instruct-FP8-Dynamic
            - ADMIN_EMAIL=jelambe@iu.edu
            - ENABLE_OLLAMA_API=TRUE
            - OPENAI_API_BASE_URL=http://vllm:8000/v1
            - OPENAI_API_KEY=Empty
        depends_on:
            - vllm
        deploy:
            resources:
                limits:
                    memory: 4G
        networks:
            - llm_network

volumes:
    open-webui:

networks:
    llm_network:
        driver: bridge
